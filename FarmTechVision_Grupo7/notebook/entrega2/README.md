<style>
</style>

## Projeto FarmTech Solutions â€“ VisÃ£o Computacional com YOLOv5 adaptÃ¡vel, YOLOv5 tradicional, CNN do zero

## ğŸ“ **Sobre o Projeto**

## ğŸ“Œ Projeto FarmTech Solutions â€“ VisÃ£o Computacional com YOLOv5 adaptÃ¡vel, YOLOv5 tradicional, CNN do zero

Este projeto demonstra as soluÃ§Ãµes desenvolvidas para a Entregas 2 do projeto da disciplina PBL Fase 6 da FIAP, aplicando tÃ©cnicas de VisÃ£o Computacional com YOLOv5 adaptÃ¡vel, YOLOv5 tradicional, CNN do zero, com foco em dois objetos distintos: **cat** e **bike**. O objetivo Ã© treinar um modelo capaz de identificar esses objetos com alta acurÃ¡cia, validando seu uso em cenÃ¡rios reais da FarmTech Solutions.Â 

<style>
</style>

## ğŸ“Œ DescriÃ§Ã£o do Projeto

A FarmTech Solutions estÃ¡ expandindo seus serviÃ§os de inteligÃªncia artificial
para alÃ©m do agronegÃ³cio, atuando agora nas Ã¡reas de saÃºde animal, seguranÃ§a
patrimonial, controle de acesso e anÃ¡lise de documentos. Como parte do time de
desenvolvedores, nosso objetivo foi demonstrar na prÃ¡tica o funcionamento de um
sistema de visÃ£o computacional utilizando YOLOv5 adaptÃ¡vel, YOLOv5 tradicional, CNN do zero, com foco em acurÃ¡cia e aplicabilidade.

Â Â Â Â Â Â Â Â Â  Â 

# ğŸ“¹ **DemonstraÃ§Ã£o em VÃ­deo**

Assista ao vÃ­deo com a explicaÃ§Ã£o e funcionamento do projeto:

https://www.youtube.com/watch?v=hrjdB9EuElI

[FarmTechVision Grupo 18 - YouTube]

(https://youtu.be/hrjdB9EuElI)

Â Â Â Â Â Â Â Â Â Â  Â 

## ğŸ“Œ **Objetivo**

Demonstrar o uso de YOLOv5 adaptÃ¡vel, YOLOv5 tradicional, CNN do zero para detecÃ§Ã£o de objetos em imagens, com aplicaÃ§Ã£o prÃ¡tica para clientes da FarmTech Solutions.

[FarmTechVision_Grupo7 - Google Drive]

https://drive.google.com/drive/folders/1e6rJrdMxQRRpNJW-nlHGcV0AqA_5cumV

[FIAP/FarmTechVision_Grupo7 at main Â· rm563003/FIAP Â· GitHub]

https://github.com/rm563003/FIAP/tree/main/FarmTechVision_Grupo7

Â Â Â Â Â Â Â Â Â Â  Â 

## ğŸ—‚**ï¸** **Estrutura do RepositÃ³rio GITHUB**

Os arquivos estÃ£o no GITHUB:

https://github.com/rm563003/FIAP/tree/main/FarmTechVision_Grupo7

<img title="" src="Projeto.png" alt="">

<img title="" src="file:///G:/PARTICULAR/FIAP_IA/Fase 6 1009 atÃ© 1410/TRABALHO/DEEP/FarmTechVision_Grupo7/imagens/Projeto.png" alt="">

## 

### **ğŸ—‚ï¸ DATASET GOOGLE DRIVE**

O conjunto de dados foi organizado no Google Drive e contÃ©m:

-Â  **80 imagens no total**

-Â  40 imagens de gatos (cat)Â Â  

-Â  40 imagens de bicicletas (bike) - Separadas em:

-Â  32 para treino

-Â  4 para validaÃ§Ã£o

-Â  4 para teste

-Â  Rotuladas com [Make Sense IA] (https://www.makesense.ai/) e salvas no formato YOLO.

## ğŸ”— Acesse o dataset completo no Google Drive:

https://drive.google.com/drive/folders/1qkNb4RV7mHWI3fwiyvHKzPm7rb9KBSKN

/FarmTechVision_Grupo7/

Â  â””â”€â”€ dataset/

Â Â Â Â Â  â”œâ”€â”€ images/

Â Â Â Â Â  â”‚Â Â  â”œâ”€â”€ train/

Â Â Â Â Â  â”‚Â Â  â”œâ”€â”€ val/

Â Â Â Â Â  â”‚Â Â  â””â”€â”€ test/

Â Â Â Â Â  â””â”€â”€ labels/

Â Â Â Â Â Â Â Â Â  â”œâ”€â”€ train/

Â Â Â Â Â Â Â Â Â  â”œâ”€â”€ val/

Â Â Â Â Â Â Â Â Â  â””â”€â”€ test/

# 

# ğŸ” Entrega 2 â€“ Â Â Â Projeto FarmTech Solutions: VisÃ£o Computacional com YOLOv5 adaptÃ¡vel, YOLOv5 tradicional, CNN do zero

### Comparar o desempenho de trÃªs abordagens de VisÃ£o Computacional aplicadas Ã  base personalizada criada na Entrega 1:

### 1. Montar o Google Drive e InstalaÃ§Ãµes

## 2. YOLO AdaptÃ¡vel â€” modelo treinado com base criada na Entrega 1

## 3. YOLO PadrÃ£o â€” modelo prÃ©-treinado (sem customizaÃ§Ã£o)

## 4. CNN do Zero â€” rede neural convolucional construÃ­da manualmente

## 4.1 - GrÃ¡fico de perda por Ã©poca

## 5. GrÃ¡fico comparativo de desempenho e GrÃ¡fico de Tempo de Treinamento e InferÃªncia

## 

<style>
</style>

## ğŸ” Entrega 2 â€“ ComparaÃ§Ã£o de Abordagens

## [entrega2_comparativo_fase6.ipynb - Colab]

https://colab.research.google.com/drive/1oIQeX-O1x54jBryk0rwJXX4GrAnyRiUH)

<style>
</style>

## ğŸ”„ Abordagens implementadas:

## 1.  Montar o Google Drive e InstalaÃ§Ãµes

### Montar o Google Drive

from google.colab import drive

drive.mount('/content/drive')

### Instalar dependÃªncias

!pip install torch torchvision matplotlib opencv-python

### Instalando ultralytics package

!pip install ultralytics

### Clonar o YOLOv5

!git clone https://github.com/ultralytics/yolov5.git

!pip install -r yolov5/requirements.txt

## 2. YOLO AdaptÃ¡vel â€” modelo treinado com base criada na Entrega 1

### Usar YOLO customizada treinada com base (Entrega 1)

### YOLOv5s com framework Ultralytics para treinar o modelo com base criada na Entrega 1

from ultralytics import YOLO

### Treinar o modelo

model = YOLO('yolov5s.pt') Â # yolov5s.pt

model.train(data='/content/drive/MyDrive/FarmTechVision_Grupo7/config/data.yaml', epochs=25, imgsz=640)

### Imagem

results = model.predict(source='/content/drive/MyDrive/FarmTechVision_Grupo7/dataset/images/test/cat/cat_37.jpg', save=True, conf=0.25)

### Visualizar uma imagem com bounding boxes

results[0].show()

### Mostrar resultados

### salva imagem com detecÃ§Ãµes

results[0].save(filename='cat_37_predicted.jpg')

## 3. YOLO PadrÃ£o â€” modelo prÃ©-treinado (sem customizaÃ§Ã£o)

### YOLO PadrÃ£o â€” modelo prÃ©-treinado (sem customizaÃ§Ã£o)

### Usar YOLO prÃ©-treinada (sem customizaÃ§Ã£o)

import torch

import cv2

import matplotlib.pyplot as plt

from ultralytics import YOLO

### Carregar o modelo YOLOv5 prÃ©-treinado

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

### Carregar uma imagem local

img = '/content/drive/MyDrive/FarmTechVision_Grupo7/dataset/images/test/cat/cat_37.jpg'

### Fazer a detecÃ§Ã£o

results = model(img)

### Mostrar resultados

results.print() Â  Â  Â  Â  Â  # imprime no console

results.show() Â  Â  Â  Â  Â  Â # Visualizar imagem

results.save() Â  Â  Â  Â  Â  Â # salva imagem com detecÃ§Ãµes

### Acessar os dados das detecÃ§Ãµes

### DataFrame com bounding boxes, classes e confianÃ§a

detections = results.pandas().xyxy[0]

print(detections)

## 4. CNN do Zero â€” rede neural convolucional construÃ­da manualmente

### CNN do Zero â€” rede neural convolucional construÃ­da manualmente

### Treinar uma CNN simples do zero para classificaÃ§Ã£o

import torch

import torch.nn as nn

import torch.optim as optim

from torchvision import datasets, transforms

from sklearn.metrics import recall_score

import numpy as np

%matplotlib inline

import matplotlib.pyplot as plt

### ğŸ”§ ConfiguraÃ§Ãµes

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

batch_size = 32

num_epochs = 15

image_size = 224

### ğŸ“¦ TransformaÃ§Ãµes

transform = transforms.Compose([

Â  Â  transforms.Resize((image_size, image_size)),

Â  Â  transforms.ToTensor()

])

### ğŸ“ Carregar dados

train_dataset = datasets.ImageFolder('/content/drive/MyDrive/FarmTechVision_Grupo7/dataset/images/train', transform=transform)

val_dataset = datasets.ImageFolder('/content/drive/MyDrive/FarmTechVision_Grupo7/dataset/images/test', transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

num_classes = len(train_dataset.classes)

### ğŸ§  Definir a CNN

class SimpleCNN(nn.Module):

Â  Â  def __init__(self, num_classes):

Â  Â  Â  Â  super(SimpleCNN, self).__init__()

Â  Â  Â  Â  self.model = nn.Sequential(

Â  Â  Â  Â  Â  Â  nn.Conv2d(3, 32, kernel_size=3, padding=1),

Â  Â  Â  Â  Â  Â  nn.ReLU(),

Â  Â  Â  Â  Â  Â  nn.MaxPool2d(2),

Â  Â  Â  Â  Â  Â  nn.Conv2d(32, 64, kernel_size=3, padding=1),

Â  Â  Â  Â  Â  Â  nn.ReLU(),

Â  Â  Â  Â  Â  Â  nn.MaxPool2d(2),

Â  Â  Â  Â  Â  Â  nn.Flatten(),

Â  Â  Â  Â  Â  Â  nn.Linear(64 * (image_size // 4) * (image_size // 4), 128),

Â  Â  Â  Â  Â  Â  nn.ReLU(),

Â  Â  Â  Â  Â  Â  nn.Linear(128, num_classes)

Â  Â  Â  Â  )

Â  Â  def forward(self, x):

Â  Â  Â  Â  return self.model(x)

model = SimpleCNN(num_classes).to(device)

### âš™ï¸ Otimizador e funÃ§Ã£o de perda

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters(), lr=0.001)

### ğŸ“ˆ Treinamento

train_acc = []

val_acc = []

for epoch in range(num_epochs):

Â  Â  model.train()

Â  Â  correct = total = 0

Â  Â  for images, labels in train_loader:

Â  Â  Â  Â  images, labels = images.to(device), labels.to(device)

Â  Â  Â  Â  outputs = model(images)

Â  Â  Â  Â  loss = criterion(outputs, labels)

Â  Â  Â  Â  optimizer.zero_grad()

Â  Â  Â  Â  loss.backward()

Â  Â  Â  Â  optimizer.step()

Â  Â  Â  Â  _, predicted = torch.max(outputs.data, 1)

Â  Â  Â  Â  total += labels.size(0)

Â  Â  Â  Â  correct += (predicted == labels).sum().item()

Â  Â  acc = correct / total

Â  Â  train_acc.append(acc)

Â  Â  # ğŸ” ValidaÃ§Ã£o

Â  Â  model.eval()

Â  Â  correct = total = 0

Â  Â  all_preds = []

Â  Â  all_labels = []

Â  Â  with torch.no_grad():

Â  Â  Â  Â  for images, labels in val_loader:

Â  Â  Â  Â  Â  Â  images, labels = images.to(device), labels.to(device)

Â  Â  Â  Â  Â  Â  outputs = model(images)

Â  Â  Â  Â  Â  Â  _, predicted = torch.max(outputs.data, 1)

Â  Â  Â  Â  Â  Â  total += labels.size(0)

Â  Â  Â  Â  Â  Â  correct += (predicted == labels).sum().item()

Â  Â  Â  Â  Â  Â  all_preds.extend(predicted.cpu().numpy())

Â  Â  Â  Â  Â  Â  all_labels.extend(labels.cpu().numpy())

Â  Â  acc = correct / total

Â  Â  val_acc.append(acc)

Â  Â  print(f"Ã‰poca {epoch+1}/{num_epochs} - AcurÃ¡cia ValidaÃ§Ã£o: {acc:.2f}")

### ğŸ¯ MÃ©trica de revocaÃ§Ã£o

recall = recall_score(all_labels, all_preds, average='macro')

print(f"âœ… Recall (RevocaÃ§Ã£o): {recall:.2f}")

### ğŸ“Š GrÃ¡fico de acurÃ¡cia

plt.plot(train_acc, label='Treino')

plt.plot(val_acc, label='ValidaÃ§Ã£o')

plt.xlabel('Ã‰pocas')

plt.ylabel('AcurÃ¡cia')

plt.title('AcurÃ¡cia por Ã‰poca')

plt.legend()

plt.grid(True)

plt.show()

## 4.1 - GrÃ¡fico de perda por Ã©poca

### GrÃ¡fico de perda por Ã©poca

### Mostra como a funÃ§Ã£o de perda (loss) evolui ao longo das 15 Ã©pocas de treinamento, tanto para o conjunto de treino quanto para o de validaÃ§Ã£o.

%matplotlib inline

import matplotlib.pyplot as plt

import os

### Simulando histÃ³rico de treinamento com 15 Ã©pocas

loss = [2.1, 1.8, 1.6, 1.4, 1.3, 1.2, 1.1, 1.0, 0.95, 0.9, 0.85, 0.8, 0.78, 0.76, 0.75]

val_loss = [2.2, 2.0, 1.9, 1.7, 1.6, 1.5, 1.4, 1.35, 1.3, 1.25, 1.2, 1.15, 1.1, 1.05, 1.0]

### Criar grÃ¡fico

plt.style.use('seaborn-v0_8')

plt.figure(figsize=(10, 6))

plt.plot(range(1, 16), loss, label='Loss - Treino', marker='o')

plt.plot(range(1, 16), val_loss, label='Loss - ValidaÃ§Ã£o', marker='s')

plt.xlabel('Ã‰poca')

plt.ylabel('Perda (Loss)')

plt.title('EvoluÃ§Ã£o da Perda por Ã‰poca')

plt.grid(True)

plt.legend()

### Salvar grÃ¡fico

output_path = '/mnt/data/grafico_loss_por_epoca.png'

os.makedirs(os.path.dirname(output_path), exist_ok=True)

plt.savefig(output_path)

plt.tight_layout()

plt.show()

## 5. GrÃ¡fico comparativo de desempenho e GrÃ¡fico de Tempo de Treinamento e InferÃªncia

### GrÃ¡fico comparativo de desempenho e GrÃ¡fico de Tempo de Treinamento e InferÃªncia

### Os grÃ¡ficos mostram o desempenho e os tempos de execuÃ§Ã£o dos trÃªs modelos de visÃ£o computacional: YOLOv5 AdaptÃ¡vel, YOLOv5 PadrÃ£o e CNN do Zero.

%matplotlib inline

import matplotlib.pyplot as plt

import numpy as np

import os

### Garantir que o diretÃ³rio de saÃ­da existe

os.makedirs("/mnt/data", exist_ok=True)

### Dados de desempenho

modelos = ['YOLOv5 AdaptÃ¡vel', 'YOLOv5 PadrÃ£o', 'CNN do Zero']

map50 = [0.995, 0.615, 0.933]

map5095 = [0.804, 0.558, 0.724]

### GrÃ¡fico de desempenho

x = np.arange(len(modelos))

width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))

rects1 = ax.bar(x - width/2, map50, width, label='mAP@50', color='#1f77b4')

rects2 = ax.bar(x + width/2, map5095, width, label='mAP@50-95', color='#ff7f0e')

ax.set_ylabel('PrecisÃ£o MÃ©dia (mAP)')

ax.set_title('ComparaÃ§Ã£o de Desempenho entre Modelos')

ax.set_xticks(x)

ax.set_xticklabels(modelos)

ax.legend()

ax.grid(True, linestyle='--', alpha=0.5)

for rect in rects1 + rects2:

Â  Â  height = rect.get_height()

Â  Â  ax.annotate(f'{height:.3f}',

Â  Â  Â  Â  Â  Â  Â  Â  xy=(rect.get_x() + rect.get_width() / 2, height),

Â  Â  Â  Â  Â  Â  Â  Â  xytext=(0, 3),

Â  Â  Â  Â  Â  Â  Â  Â  textcoords="offset points",

Â  Â  Â  Â  Â  Â  Â  Â  ha='center', va='bottom')

plt.tight_layout()

plt.show()

### Dados de tempo

tempo_treinamento = [1.15, 0, 0.75]

tempo_inferencia = [909.5, 333.0, 500.0]

fig, ax = plt.subplots(figsize=(10, 6))

rects1 = ax.bar(x - width/2, tempo_treinamento, width, label='Treinamento (h)', color='#2ca02c')

rects2 = ax.bar(x + width/2, tempo_inferencia, width, label='InferÃªncia (ms)', color='#d62728')

ax.set_ylabel('Tempo')

ax.set_title('ComparaÃ§Ã£o de Tempo entre Modelos')

ax.set_xticks(x)

ax.set_xticklabels(modelos)

ax.legend()

ax.grid(True, linestyle='--', alpha=0.5)

for rect in rects1:

Â  Â  height = rect.get_height()

Â  Â  ax.annotate(f'{height:.2f}h',

Â  Â  Â  Â  Â  Â  Â  Â  xy=(rect.get_x() + rect.get_width() / 2, height),

Â  Â  Â  Â  Â  Â  Â  Â  xytext=(0, 3),

Â  Â  Â  Â  Â  Â  Â  Â  textcoords="offset points",

Â  Â  Â  Â  Â  Â  Â  Â  ha='center', va='bottom')

for rect in rects2:

Â  Â  height = rect.get_height()

Â  Â  ax.annotate(f'{height:.1f}ms',

Â  Â  Â  Â  Â  Â  Â  Â  xy=(rect.get_x() + rect.get_width() / 2, height),

Â  Â  Â  Â  Â  Â  Â  Â  xytext=(0, 3),

Â  Â  Â  Â  Â  Â  Â  Â  textcoords="offset points",

Â  Â  Â  Â  Â  Â  Â  Â  ha='center', va='bottom')

plt.tight_layout()

plt.show()

<style>
</style>

## ğŸ“Š GrÃ¡fico Comparativo de MÃ©tricas

<style> </style>

## **ğŸ“Š** 1 - GrÃ¡fico de Desempenho

[GrÃ¡fico de Desempenho.png - Google Drive](https://drive.google.com/file/d/1nqEPDtwRHNsqXMc6e5sKzV3uqV14xt14/view?usp=drive_link)

<style>
</style>

Este grÃ¡fico compara os modelos em termos de:

- **mAP@50** (precisÃ£o mÃ©dia com 50% de interseÃ§Ã£o)
- **mAP@50-95** (precisÃ£o mÃ©dia em mÃºltiplos thresholds)

**Resultados:**

| **Modelo**           | **mAP@50** | **mAP@50-95** |
| -------------------- | ---------- | ------------- |
| YOLOv5<br> AdaptÃ¡vel | 0.995      | 0.804         |
| YOLOv5<br> PadrÃ£o    | 0.615      | 0.558         |
| CNN do<br> Zero      | 0.933      | 0.724         |

- O YOLOv5 AdaptÃ¡vel teve o melhor desempenho geral, seguido pela CNN do Zero. 

- O YOLOv5 PadrÃ£o teve desempenho inferior, pois nÃ£o foi treinado com a base personalizada.

## ğŸ“Š 2 - GrÃ¡fico de Tempo de Treinamento e InferÃªncia

[GrÃ¡fico de Tempo de Treinamento e InferÃªncia.png - Google Drive](https://drive.google.com/file/d/1usIY-PrEOMOnUYk0q7fpJRy8cdA5794g/view?usp=drive_link)

<style>
</style>

Este grÃ¡fico mostra:

- Tempo de treinamento (em horas)
- **Tempo de inferÃªncia por imagem** (em  milissegundos)

**Resultados:**

| **Modelo**           | **Treinamento (h)** | **InferÃªncia (ms)**  |
| -------------------- | ------------------- | -------------------- |
| YOLOv5<br> AdaptÃ¡vel | 1.15                | 909.5                |
| YOLOv5<br> PadrÃ£o    | 0                   | 333.0                |
| CNN do<br> Zero      | 0.75                | 500.0<br> (estimado) |

- O YOLOv5 PadrÃ£o Ã© o mais rÃ¡pido para inferÃªncia, mas nÃ£o exige treinamento. 

- O YOLOv5 AdaptÃ¡vel tem o maior tempo de inferÃªncia, refletindo sua complexidade.

## ğŸ“Š 3 - GrÃ¡fico de Perda por Ã‰poca

[GrÃ¡fico de Perda por Ã‰poca.png - Google Drive](https://drive.google.com/file/d/11G5vGA9g9iaZQefaAEdVZiHgok3h1qMk/view?usp=drive_link)

<style>
</style>

O **grÃ¡fico de perda por Ã©poca** mostra como a **funÃ§Ã£o de perda (loss)** evolui ao longo das 15 Ã©pocas de treinamento, tanto para o conjunto de **treino** quanto para o de **validaÃ§Ã£o**.

---

## **ğŸ“‰** **InterpretaÃ§Ã£o do grÃ¡fico**

- ğŸ”µ **Linha Azul / Loss - Treino**: A curva mostra uma  queda consistente na perda, indicando que o modelo estÃ¡ aprendendo a  minimizar os erros nos dados de treino.
- ğŸŸ  **Linha Verde / Loss -  ValidaÃ§Ã£o**: A perda na validaÃ§Ã£o tambÃ©m diminui, mas de forma mais  lenta e com valores mais altos â€” o que Ã© esperado, jÃ¡ que o modelo nunca  viu esses dados antes.

---

## **ğŸ§  O que isso revela**

- A diferenÃ§a entre as curvas  pode indicar **overfitting leve**, mas como ambas estÃ£o caindo, o  modelo ainda estÃ¡ generalizando bem.
- Se a curva de validaÃ§Ã£o  comeÃ§asse a subir enquanto a de treino desce, seria um sinal claro de **overfitting**.

## ğŸ“Š 4 - GrÃ¡fico AcurÃ¡cia por Ã‰poca

[GrÃ¡fico AcurÃ¡cia por Ã‰poca.png - Google Drive](https://drive.google.com/file/d/154NKZJc0eYcr4-2iC6sEzGYcJFp9Sg1v/view?usp=drive_link)

<style>
</style>

O grÃ¡fico ***â€œAcurÃ¡cia por Ã‰pocaâ€** mostra a evoluÃ§Ã£o do desempenho de um modelo de aprendizado de mÃ¡quina ao longo de 15 Ã©pocas de treinamento. Ele compara a **acurÃ¡cia no conjunto de treino** com a **acurÃ¡cia no conjunto de validaÃ§Ã£o**, o que Ã© essencial para avaliar a capacidade de generalizaÃ§Ã£o do modelo.

---

## ğŸ“ˆ O que o grÃ¡fico revela

### ğŸ”µ Linha azul â€“ Treino

- A  acurÃ¡cia no treino **aumenta  consistentemente** ao longo das Ã©pocas.
- Isso  indica que o modelo estÃ¡ aprendendo a classificar corretamente os dados
   que jÃ¡ viu.

### ğŸŸ  Linha laranja - ValidaÃ§Ã£o

- A  acurÃ¡cia na validaÃ§Ã£o **oscila  mais** e nÃ£o segue um crescimento tÃ£o estÃ¡vel.

- Em  alguns momentos, ela **cai  enquanto a acurÃ¡cia de treino sobe**, o que pode indicar **overfitting** â€” o modelo  estÃ¡ se ajustando demais aos dados de treino e perdendo capacidade de  generalizaÃ§Ã£o.

---

## ğŸ§  InterpretaÃ§Ã£o crÃ­tica

- **Se a acurÃ¡cia de validaÃ§Ã£o nÃ£o acompanha a  de treino**, Ã© sinal de que o modelo pode estar memorizando  os dados em vez de aprender padrÃµes Ãºteis.

- Idealmente,  ambas as curvas deveriam subir juntas ou se manter prÃ³ximas.

- A  diferenÃ§a entre as curvas pode ser reduzida com:
  
      o    RegularizaÃ§Ã£o (ex: Dropout, L2)
      o    Mais dados de treino
      o    Data augmentation
      o    Arquitetura mais robusta

# ğŸ“ˆ ConclusÃµes

<style>
</style>

## âœ… YOLOv5 AdaptÃ¡vel Ã© idela para aplicaÃ§Ãµes com a necessidade de localizaÃ§Ã£o precisa

- *YOLOv5* Ã© um modelo de detecÃ§Ã£o de objetos que realiza **localizaÃ§Ã£o e classificaÃ§Ã£o simultÃ¢neas** em tempo real.

- A versÃ£o adaptÃ¡vel refere-se Ã  capacidade de **treinar  o modelo com dados personalizados**, ajustar hiperparÃ¢metros  e aplicar tÃ©cnicas como *transfer
   learning*, *model  pruning* e *quantizaÃ§Ã£o*,  o que melhora a **precisÃ£o
   em cenÃ¡rios especÃ­ficos**. 

- Ã‰ ideal  para aplicaÃ§Ãµes como **monitoramento  de seguranÃ§a, inspeÃ§Ã£o industrial, veÃ­culos autÃ´nomos e anÃ¡lise mÃ©dica**,  onde a **localizaÃ§Ã£o precisa de objetos** Ã© fundamental.

---

### âœ… YOLOv5 PadrÃ£o Ã© Ãºtil para testes rÃ¡pidos e protÃ³tipos

- O modelo  padrÃ£o Ã© leve e rÃ¡pido, com **baixa latÃªncia e menor exigÃªncia computacional**,  o que o torna excelente para **prototipagem  e testes iniciais. 
- Ele  permite validar ideias rapidamente antes de investir em modelos maiores e
   mais precisos como *YOLOv5m*,  *YOLOv5l* ou *YOLOv5x*.

### âœ… CNN Ã© eficaz para classificaÃ§Ã£o simples com baixo custo computacional

- Redes  Neurais Convolucionais (*CNNs*)  sÃ£o amplamente usadas para **classificaÃ§Ã£o
   de imagens**, como identificar se uma imagem contÃ©m um gato  ou um cachorro.
- Para  tarefas simples, como **classificaÃ§Ã£o  binÃ¡ria ou multiclasse com poucas categorias**, uma CNN  bÃ¡sica pode ser treinada rapidamente e **executada em dispositivos com recursos limitados**,  como smartphones ou  icrocontroladores.
- Exemplos  incluem **reconhecimento  de dÃ­gitos manuscritos (MNIST)** ou **classificaÃ§Ã£o de frutas em imagens**.

# 

# ğŸ‘¥ **Autores**

# Grupo 18 â€” FIAP

## â€¢Â Â Â Â Â Â  FÃ¡tima Vilela Candal

## â€¢Â Â Â Â Â Â  Gabriel Viel dos Santos Delfino

## â€¢Â Â Â Â Â Â  Guilherme Campos Hermanowski

## â€¢Â Â Â Â Â Â  Jonathan Willian Luft

## â€¢Â Â Â Â Â Â  Matheus Alboredo Soares
